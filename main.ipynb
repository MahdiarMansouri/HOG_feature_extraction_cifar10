{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T09:12:50.896493100Z",
     "start_time": "2024-02-02T09:12:50.855492900Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage.exposure import exposure\n",
    "from skimage.feature import hog\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PreParing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef01ecaf5323313"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations will be on your: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data transforms\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.255]\n",
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Operations will be on your: {device}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T09:12:50.942493400Z",
     "start_time": "2024-02-02T09:12:50.865492800Z"
    }
   },
   "id": "7727b44ab60a0ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Class names are ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "DataLoaders Are Ready.\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "datasets = {\n",
    "    'train': torchvision.datasets.CIFAR10(root='./data', train=True, transform=data_transform['train'],\n",
    "                                          download=True),\n",
    "    'val': torchvision.datasets.CIFAR10(root='./data', train=False, transform=data_transform['val'],\n",
    "                                        download=True)\n",
    "}\n",
    "\n",
    "# Defining class names\n",
    "class_names = datasets['train'].classes\n",
    "print(f'Class names are {class_names}')\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 1\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset=datasets[x], batch_size=batch_size, shuffle=False, num_workers=2) for x in\n",
    "               ['train', 'val']}\n",
    "\n",
    "print('DataLoaders Are Ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T09:12:52.103036400Z",
     "start_time": "2024-02-02T09:12:50.878492800Z"
    }
   },
   "id": "60568c325b77e1a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting HOG Features and Making New Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8a442711aff5ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start HOG extracting form train images in: 2024-02-02 12:42:52.100036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000228A839B5B0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1436, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "orientations = 8\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (1, 1)\n",
    "block_norm = 'L2-Hys'\n",
    "\n",
    "# Make HOG features dataset \n",
    "def extract_hog_features(image):\n",
    "    hog_channels = []\n",
    "    for channel in range(image.shape[0]):  \n",
    "        # Extract the single channel\n",
    "        single_channel = image[channel, :, :].cpu().numpy()\n",
    "\n",
    "        # Extract HOG features for the single channel and rescale features \n",
    "        fd, hog_channel = hog(single_channel,\n",
    "                              orientations=orientations,\n",
    "                              pixels_per_cell=pixels_per_cell,\n",
    "                              cells_per_block=cells_per_block,\n",
    "                              visualize=True)\n",
    "        hog_channel = exposure.rescale_intensity(hog_channel, in_range=(0, 10))\n",
    "        hog_channels.append(hog_channel)\n",
    "\n",
    "    # Stack the HOG images to form a 3-channel image\n",
    "    hog_image = np.stack(hog_channels, axis=0)\n",
    "    return torch.tensor(hog_image, dtype=torch.float32)\n",
    "\n",
    "# Define HOG extracted datasets\n",
    "hog_datasets = {\n",
    "    'train': None,\n",
    "    'val': None\n",
    "}\n",
    "\n",
    "# Set number of new dataset images to Subset from original dataset\n",
    "datasets_length = 25000\n",
    "\n",
    "# Iterating over two datasets\n",
    "for phase in ['train', 'val']:\n",
    "    # Set timer for extracting HOG\n",
    "    s0 = datetime.datetime.now()\n",
    "    print(f'start HOG extracting form {phase} images in: {s0}')\n",
    "\n",
    "    # Define lists for hog features \n",
    "    hog_images = []\n",
    "    labels = []\n",
    "    \n",
    "    # define counter for datasets length\n",
    "    counter = 0\n",
    "    \n",
    "    # Iterating over batches and data for HOG feature extraction\n",
    "    for image, label in dataloaders[phase]:\n",
    "        # Extract HOG features for each image\n",
    "        hog_image = extract_hog_features(image.squeeze())  # Squeeze in case the batch size is 1\n",
    "        hog_images.append(hog_image)\n",
    "        labels.append(label)\n",
    "        \n",
    "        # check counter for length\n",
    "        counter += 1\n",
    "        if counter > datasets_length:\n",
    "            break\n",
    "\n",
    "    # Convert lists of tensors to a single tensor   \n",
    "    hog_dataset = torch.stack(hog_images)\n",
    "    labels = torch.cat(labels)\n",
    "        \n",
    "    # Create a new dataset with HOG images and labels\n",
    "    hog_datasets[phase] = TensorDataset(hog_dataset, labels)\n",
    "    print(f'Extracting Done in: {datetime.datetime.now() - s0}')\n",
    "\n",
    "\n",
    "print(len(hog_datasets['train']))\n",
    "print(len(hog_datasets['val']))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-02T09:12:52.096036200Z"
    }
   },
   "id": "1f26ba703bef8595"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make new small datasets for comparing HOG vs Original images\n",
    "indices = list(range(datasets_length))\n",
    "for dataset in [datasets, hog_datasets]:\n",
    "    for phase in ['train', 'val']:\n",
    "        dataset[phase] = Subset(dataset[phase], indices)\n",
    "    \n",
    "# Create new small Dataloaders\n",
    "batch_size = 32\n",
    "original_dataloaders = {x: torch.utils.data.DataLoader(dataset=datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in\n",
    "               ['train', 'val']}\n",
    "\n",
    "hog_dataloaders = {x: torch.utils.data.DataLoader(dataset=hog_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in\n",
    "               ['train', 'val']}"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "444a451de0870a63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "\n",
    "print(len(hog_datasets['train']))\n",
    "print(len(hog_datasets['val']))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2d48635b4794b336"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9d6bdd036e1839"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all trainable layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modifying last classification layer for our dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "# print(pretrained_model)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "628d3132ed48a91a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1a5dbebafb55c0bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b4b8f5d5246d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train function \n",
    "def train_model(model, criterion, optimizer, model_dataloaders, model_datasets,  epoch_num=25):\n",
    "    acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "    loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "    \n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            # Define starting time\n",
    "            s0 = datetime.datetime.now()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over batches and data for training and validation\n",
    "            for idx, batch in enumerate(model_dataloaders[phase], 0):\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Transfer data and labels to CUDA if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # Back Propagation and updating weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "            \n",
    "            # Calculating Accuracy and Loss per phase\n",
    "            epoch_loss = running_loss / len(model_datasets[phase])\n",
    "            epoch_accuracy = running_corrects.double() / len(model_datasets[phase])\n",
    "            \n",
    "            # Show epoch details\n",
    "            print(f'{phase.capitalize()} Accuracy: {epoch_accuracy:.4f} ||| Loss: {epoch_loss:.4f}', end=' ')\n",
    "            \n",
    "            # Calculate each phase duration time\n",
    "            s1 = datetime.datetime.now()\n",
    "            delta_time = s1 - s0\n",
    "            print(f' --> duration: {delta_time}')\n",
    "            \n",
    "            # Copy the model weights if its better\n",
    "            if phase == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "                print('Best model weights updated!')\n",
    "                \n",
    "            # Save Loss and accuracy\n",
    "            acc_list[phase].append(epoch_accuracy)\n",
    "            loss_list[phase].append(epoch_loss)        \n",
    "            \n",
    "        print()\n",
    "        \n",
    "    print(f'*** Best Accuracy: {best_accuracy:.4f} ***')\n",
    "    \n",
    "    # Loading best model weights \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_list, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eb32b0231b91f654"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = pretrained_model.to(device)\n",
    "\n",
    "# Train model\n",
    "s0 = datetime.datetime.now()\n",
    "model, acc_list, loss_list = train_model(model, criterion, optimizer, original_dataloaders, datasets, epoch_num=100)\n",
    "delta_time = datetime.datetime.now() - s0\n",
    "print(f'Total training duration: {delta_time}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cb0e0f22f93f88a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot([a.cpu() for a in acc_list.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_list.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3799f650fee2907f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot([a for a in loss_list.train], label='train')\n",
    "plt.plot([a for a in loss_list.val], label='val')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "42dc3e7a0cc6e958"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29e08f9500d8858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_cm(model):\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in original_dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "            y_pred.extend(outputs)\n",
    "\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cm / np.sum(cm, axis=1)[:, None],\n",
    "        index=[i for i in class_names],\n",
    "        columns=[i for i in class_names]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(df_cm, annot=True, cbar=False)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7180ffa0e069b94a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_cm(model)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e8fa9f5000c7e6c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing The Predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39bfbfbc01511638"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(original_dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8e87e2ab6b447c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "81b9050f4b251a14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Do Everything For HOG Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63c521bf29cd70ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all trainable layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modifying last classification layer for our dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)\n",
    "# print(pretrained_model)\n",
    "\n",
    "model2 = pretrained_model.to(device)\n",
    "\n",
    "# Train model\n",
    "model2, acc_list2, loss_list2 = train_model(model2, criterion, optimizer, hog_dataloaders, hog_datasets, epoch_num=100)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "328c72f7e04ea45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_list2.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_list2.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "676e4cbc8ed368dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_list2.train], label='train')\n",
    "plt.plot([a for a in loss_list2.val], label='val')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f4515b33440421b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_cm(model2)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "698ce7d3f2ad36ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_model(model2)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a1780ac13bcb513f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
