{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:48.233561900Z",
     "start_time": "2024-01-11T19:51:48.213562100Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage.exposure import exposure\n",
    "from skimage.feature import hog\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PreParing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef01ecaf5323313"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data transforms\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.255]\n",
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "    ])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:48.961571600Z",
     "start_time": "2024-01-11T19:51:48.944799400Z"
    }
   },
   "id": "7727b44ab60a0ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Class names are ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "DataLoaders Are Ready.\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "datasets = {\n",
    "    'train': torchvision.datasets.CIFAR10(root='./data', train=True, transform=data_transform['train'],\n",
    "                                          download=True),\n",
    "    'val': torchvision.datasets.CIFAR10(root='./data', train=False, transform=data_transform['val'],\n",
    "                                        download=True)\n",
    "}\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=data_transform['train'],\n",
    "#                                              download=True)\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=data_transform['val'], download=True)\n",
    "\n",
    "# Defining class names\n",
    "class_names = datasets['train'].classes\n",
    "print(f'Class names are {class_names}')\n",
    "\n",
    "# Creating DataLoaders\n",
    "dataloaders = {x: torch.utils.DataLoader(dataset=datasets[x], batch_size= 32, shuffle=True, num_workers=2) for x in ['train', 'val']}\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "print('DataLoaders Are Ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:50.430540400Z",
     "start_time": "2024-01-11T19:51:49.198368300Z"
    }
   },
   "id": "60568c325b77e1a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOG Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7b021d72900b68"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# HOG Parameters\n",
    "orientations = 9\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (2, 2)\n",
    "block_norm = 'L2-Hys'\n",
    "\n",
    "\n",
    "class HOGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HOGFeatureExtractor, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hog_features_list = []\n",
    "        # Iterating over data in each batch and extract hog features \n",
    "        for image in x:\n",
    "            # Convert Image to cpu tensor and numpy for hog\n",
    "            image = image.cpu()\n",
    "            np_image = image.numpy().squeeze()\n",
    "\n",
    "            # Extract HOG features from each channel \n",
    "            img_channels = []\n",
    "            for channel_num in range(np_image.shape[2]):\n",
    "                hog_channel = hog(\n",
    "                    np_image[:, :, channel_num],\n",
    "                    orientations=orientations,\n",
    "                    pixels_per_cell=pixels_per_cell,\n",
    "                    cells_per_block=cells_per_block,\n",
    "                    block_norm=block_norm\n",
    "                )\n",
    "                hog_channel = exposure.rescale_intensity(hog_channel, in_range=(0, 10))\n",
    "                img_channels.append(hog_channel)\n",
    "\n",
    "            img_hog = np.dstack((np.array(img_channels[0]), np.array(img_channels[1]), np.array(img_channels[2])))\n",
    "            hog_features_list.append(img_hog)\n",
    "\n",
    "        return torch.tensor(np.array(hog_features_list), dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:53:19.882669900Z",
     "start_time": "2024-01-11T19:53:19.856669400Z"
    }
   },
   "id": "18cb594d33240960"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9d6bdd036e1839"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all trainable layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modifying last classification layer for our dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.937081900Z",
     "start_time": "2024-01-10T18:47:59.843395700Z"
    }
   },
   "id": "628d3132ed48a91a"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# Defining loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.952033Z",
     "start_time": "2024-01-10T18:47:59.938079Z"
    }
   },
   "id": "1a5dbebafb55c0bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b4b8f5d5246d2d"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "hog_features_extractor = HOGFeatureExtractor()\n",
    "\n",
    "acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epoch_num=25):\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over data for training and validation\n",
    "            for idx, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Transfer data and labels to Cuda if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Extract HOG features\n",
    "                hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(hog_features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.968975300Z",
     "start_time": "2024-01-10T18:47:59.953029700Z"
    }
   },
   "id": "91d1322ab091488d"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "hog_features_extractor = HOGFeatureExtractor()\n",
    "\n",
    "acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epoch_num=25):\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        ### Training Phase ###\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # For calculating Loss and Accuracy at the end of epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        # Iterating over training data\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Transfer data and labels to Cuda if is available\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract HOG features\n",
    "            hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Backpropagation and updating weights   \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        # Show train details        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "\n",
    "        # Save loss and accuracy\n",
    "        acc_list['train'].append(train_acc)\n",
    "        loss_list['train'].append(train_loss)\n",
    "\n",
    "        ### Validation Phase ###\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # For calculating Loss and Accuracy at the end of epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        # Iterating over validation data\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Transfer data and labels to Cuda if is available\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract HOG features\n",
    "            # hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "            # Forward Pass\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(test_dataset)\n",
    "        val_acc = running_corrects.double() / len(test_dataset)\n",
    "\n",
    "        # Show details\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}', end='\\n')\n",
    "\n",
    "        # Save loss and accuracy\n",
    "        acc_list['val'].append(val_acc)\n",
    "        loss_list['val'].append(val_loss)\n",
    "\n",
    "        # Copy model weights if the weights are better \n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print('Best model weights based on Higher Val_acc updated!', end='\\n')\n",
    "\n",
    "    # Load best model weights\n",
    "    print('Loading best model weights')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.982928900Z",
     "start_time": "2024-01-10T18:47:59.973959100Z"
    }
   },
   "id": "9388a2bd96b679aa"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "Train Loss: 1.1126, Train Accuracy: 0.6208\n",
      "Validation Loss: 0.9131, Validation Accuracy: 0.6865\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.9429, Train Accuracy: 0.6731\n",
      "Validation Loss: 0.8886, Validation Accuracy: 0.6884\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.9180, Train Accuracy: 0.6818\n",
      "Validation Loss: 0.8917, Validation Accuracy: 0.6877\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.9076, Train Accuracy: 0.6866\n",
      "Validation Loss: 0.9079, Validation Accuracy: 0.6899\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 5/100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000293013CFF40>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m pretrained_model\u001B[38;5;241m.\u001B[39mto(device)    \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[87], line 46\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, epoch_num)\u001B[0m\n\u001B[0;32m     43\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     44\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 46\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     47\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(predictions \u001B[38;5;241m==\u001B[39m labels\u001B[38;5;241m.\u001B[39mdata)\n\u001B[0;32m     49\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataset)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = pretrained_model.to(device)\n",
    "\n",
    "# Train model\n",
    "model = train_model(model, criterion, optimizer, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:05.271109700Z",
     "start_time": "2024-01-10T18:47:59.983925400Z"
    }
   },
   "id": "cb0e0f22f93f88a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-10T18:52:05.241210700Z"
    }
   },
   "id": "806d008effe3383"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
