{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:48.233561900Z",
     "start_time": "2024-01-11T19:51:48.213562100Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage.exposure import exposure\n",
    "from skimage.feature import hog\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PreParing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef01ecaf5323313"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data transforms\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.255]\n",
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "    ])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:48.961571600Z",
     "start_time": "2024-01-11T19:51:48.944799400Z"
    }
   },
   "id": "7727b44ab60a0ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Class names are ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "DataLoaders Are Ready.\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=data_transform['train'],\n",
    "                                             download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=data_transform['val'], download=True)\n",
    "\n",
    "# Defining class names\n",
    "class_names = train_dataset.classes\n",
    "print(f'Class names are {class_names}')\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "print('DataLoaders Are Ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:51:50.430540400Z",
     "start_time": "2024-01-11T19:51:49.198368300Z"
    }
   },
   "id": "60568c325b77e1a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOG Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7b021d72900b68"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# HOG Parameters\n",
    "orientations = 9 \n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (2, 2)\n",
    "block_norm = 'L2-Hys'\n",
    "\n",
    "\n",
    "class HOGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HOGFeatureExtractor, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hog_features_list = []\n",
    "        # Iterating over data in each batch and extract hog features \n",
    "        for image in x:\n",
    "            # Convert Image to cpu tensor and numpy for hog\n",
    "            # image = image.cpu()\n",
    "            # np_image = image.numpy().squeeze()\n",
    "            np_image = image\n",
    "\n",
    "            # Extract HOG features from each channel \n",
    "            img_channels = []\n",
    "            for channel_num in range(3):\n",
    "                hog_channel = hog(\n",
    "                    np_image[:, :, channel_num],\n",
    "                    orientations=orientations,\n",
    "                    pixels_per_cell=pixels_per_cell,\n",
    "                    cells_per_block=cells_per_block,\n",
    "                    block_norm=block_norm\n",
    "                )\n",
    "                hog_channel = exposure.rescale_intensity(hog_channel, in_range=(0, 10))\n",
    "                img_channels.append(hog_channel)\n",
    "\n",
    "            img_hog = np.dstack((np.array(img_channels[0]), np.array(img_channels[1]), np.array(img_channels[2])))\n",
    "            hog_features_list.append(img_hog)\n",
    "\n",
    "        return torch.tensor(np.array(hog_features_list), dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:53:19.882669900Z",
     "start_time": "2024-01-11T19:53:19.856669400Z"
    }
   },
   "id": "18cb594d33240960"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         ...,\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082]],\n",
      "\n",
      "        [[-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         ...,\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082]],\n",
      "\n",
      "        [[-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         [-0.9599, -0.9599, -0.9599,  ...,  0.2029,  0.2029,  0.2029],\n",
      "         ...,\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082],\n",
      "         [ 0.5797,  0.5797,  0.5797,  ..., -0.3082, -0.3082, -0.3082]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m h \u001B[38;5;241m=\u001B[39m HOGFeatureExtractor()\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_dataset[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mh\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(img)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[20], line 24\u001B[0m, in \u001B[0;36mHOGFeatureExtractor.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m img_channels \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m channel_num \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m---> 24\u001B[0m     hog_channel \u001B[38;5;241m=\u001B[39m \u001B[43mhog\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnp_image\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannel_num\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43morientations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morientations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpixels_per_cell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixels_per_cell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcells_per_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcells_per_block\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mblock_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_norm\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     hog_channel \u001B[38;5;241m=\u001B[39m exposure\u001B[38;5;241m.\u001B[39mrescale_intensity(hog_channel, in_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m     32\u001B[0m     img_channels\u001B[38;5;241m.\u001B[39mappend(hog_channel)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\_shared\\utils.py:316\u001B[0m, in \u001B[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    313\u001B[0m channel_axis \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel_axis\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m channel_axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001B[39;00m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001B[39;00m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001B[39;00m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(channel_axis):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\feature\\_hog.py:279\u001B[0m, in \u001B[0;36mhog\u001B[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001B[0m\n\u001B[0;32m    277\u001B[0m n_blocks_row \u001B[38;5;241m=\u001B[39m (n_cells_row \u001B[38;5;241m-\u001B[39m b_row) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    278\u001B[0m n_blocks_col \u001B[38;5;241m=\u001B[39m (n_cells_col \u001B[38;5;241m-\u001B[39m b_col) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 279\u001B[0m normalized_blocks \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_blocks_row\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_blocks_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_row\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morientations\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    281\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfloat_dtype\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_blocks_row):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_blocks_col):\n",
      "\u001B[1;31mValueError\u001B[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "h = HOGFeatureExtractor()\n",
    "print(train_dataset[0][0])\n",
    "img = h([train_dataset[0][0]])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:54:32.526482800Z",
     "start_time": "2024-01-11T19:54:32.448482400Z"
    }
   },
   "id": "3c303dcf58d4d2e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9d6bdd036e1839"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all trainable layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modifying last classification layer for our dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.937081900Z",
     "start_time": "2024-01-10T18:47:59.843395700Z"
    }
   },
   "id": "628d3132ed48a91a"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# Defining loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.952033Z",
     "start_time": "2024-01-10T18:47:59.938079Z"
    }
   },
   "id": "1a5dbebafb55c0bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b4b8f5d5246d2d"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "hog_features_extractor = HOGFeatureExtractor()\n",
    "\n",
    "acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epoch_num=25):\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over data for training and validation\n",
    "            for idx, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Transfer data and labels to Cuda if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Extract HOG features\n",
    "                hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(hog_features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.968975300Z",
     "start_time": "2024-01-10T18:47:59.953029700Z"
    }
   },
   "id": "91d1322ab091488d"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "hog_features_extractor = HOGFeatureExtractor()\n",
    "\n",
    "acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epoch_num=25):\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        ### Training Phase ###\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # For calculating Loss and Accuracy at the end of epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        # Iterating over training data\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Transfer data and labels to Cuda if is available\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract HOG features\n",
    "            hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Backpropagation and updating weights   \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        # Show train details        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "\n",
    "        # Save loss and accuracy\n",
    "        acc_list['train'].append(train_acc)\n",
    "        loss_list['train'].append(train_loss)\n",
    "\n",
    "        ### Validation Phase ###\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # For calculating Loss and Accuracy at the end of epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        # Iterating over validation data\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Transfer data and labels to Cuda if is available\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract HOG features\n",
    "            # hog_features = hog_features_extractor(inputs)\n",
    "\n",
    "            # Forward Pass\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(test_dataset)\n",
    "        val_acc = running_corrects.double() / len(test_dataset)\n",
    "\n",
    "        # Show details\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}', end='\\n')\n",
    "\n",
    "        # Save loss and accuracy\n",
    "        acc_list['val'].append(val_acc)\n",
    "        loss_list['val'].append(val_loss)\n",
    "\n",
    "        # Copy model weights if the weights are better \n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print('Best model weights based on Higher Val_acc updated!', end='\\n')\n",
    "\n",
    "    # Load best model weights\n",
    "    print('Loading best model weights')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:47:59.982928900Z",
     "start_time": "2024-01-10T18:47:59.973959100Z"
    }
   },
   "id": "9388a2bd96b679aa"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "Train Loss: 1.1126, Train Accuracy: 0.6208\n",
      "Validation Loss: 0.9131, Validation Accuracy: 0.6865\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.9429, Train Accuracy: 0.6731\n",
      "Validation Loss: 0.8886, Validation Accuracy: 0.6884\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.9180, Train Accuracy: 0.6818\n",
      "Validation Loss: 0.8917, Validation Accuracy: 0.6877\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.9076, Train Accuracy: 0.6866\n",
      "Validation Loss: 0.9079, Validation Accuracy: 0.6899\n",
      "Best model weights based on Higher Val_acc updated!\n",
      "Epoch 5/100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000293013CFF40>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m pretrained_model\u001B[38;5;241m.\u001B[39mto(device)    \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[87], line 46\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, epoch_num)\u001B[0m\n\u001B[0;32m     43\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     44\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 46\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     47\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(predictions \u001B[38;5;241m==\u001B[39m labels\u001B[38;5;241m.\u001B[39mdata)\n\u001B[0;32m     49\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataset)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = pretrained_model.to(device)\n",
    "\n",
    "# Train model\n",
    "model = train_model(model, criterion, optimizer, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:05.271109700Z",
     "start_time": "2024-01-10T18:47:59.983925400Z"
    }
   },
   "id": "cb0e0f22f93f88a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-10T18:52:05.241210700Z"
    }
   },
   "id": "806d008effe3383"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
