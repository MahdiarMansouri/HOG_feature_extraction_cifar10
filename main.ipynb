{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:20:20.673898300Z",
     "start_time": "2024-01-20T15:20:20.646898200Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage.exposure import exposure\n",
    "from skimage.feature import hog\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PreParing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef01ecaf5323313"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data transforms\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.255]\n",
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        # transforms.Grayscale(num_output_channels=3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        # transforms.Grayscale(num_output_channels=3)\n",
    "    ])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T14:52:27.877629700Z",
     "start_time": "2024-01-20T14:52:27.854630200Z"
    }
   },
   "id": "7727b44ab60a0ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Class names are ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "DataLoaders Are Ready.\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "datasets = {\n",
    "    'train': torchvision.datasets.CIFAR10(root='./data', train=True, transform=data_transform['train'],\n",
    "                                          download=True),\n",
    "    'val': torchvision.datasets.CIFAR10(root='./data', train=False, transform=data_transform['val'],\n",
    "                                        download=True)\n",
    "}\n",
    "\n",
    "# Defining class names\n",
    "class_names = datasets['train'].classes\n",
    "print(f'Class names are {class_names}')\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 1\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset=datasets[x], batch_size=batch_size, shuffle=False, num_workers=2) for x in\n",
    "               ['train', 'val']}\n",
    "\n",
    "print('DataLoaders Are Ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T14:52:39.356909Z",
     "start_time": "2024-01-20T14:52:38.103906200Z"
    }
   },
   "id": "60568c325b77e1a4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start HOG extracting form train images in: 2024-01-20 18:22:56.945479\n",
      "Extracting Done in 0:05:46.783232\n",
      "start HOG extracting form val images in: 2024-01-20 18:28:43.728711\n",
      "Extracting Done in 0:06:53.145554\n",
      "1001\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Make HOG features dataset \n",
    "def extract_hog_features(image):\n",
    "    hog_channels = []\n",
    "    for channel in range(image.shape[0]):  \n",
    "        # Extract the single channel\n",
    "        single_channel = image[channel, :, :].cpu().numpy()\n",
    "\n",
    "        # Extract HOG features for the single channel\n",
    "        fd, hog_channel = hog(single_channel, pixels_per_cell=(8, 8),\n",
    "                              cells_per_block=(2, 2), visualize=True)\n",
    "        hog_channels.append(hog_channel)\n",
    "\n",
    "    # Stack the HOG images to form a 3-channel image\n",
    "    hog_image = np.stack(hog_channels, axis=0)\n",
    "    return torch.tensor(hog_image, dtype=torch.float32)\n",
    "\n",
    "hog_datasets = {\n",
    "    'train': None,\n",
    "    'val': None\n",
    "}\n",
    "\n",
    "datasets_length = 1000\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    s0 = datetime.datetime.now()\n",
    "    print(f'start HOG extracting form {phase} images in: {s0}')\n",
    "\n",
    "    hog_images = []\n",
    "    labels = []\n",
    "    c = 0\n",
    "    for image, label in dataloaders[phase]:\n",
    "        # Extract HOG features for each image\n",
    "        hog_image = extract_hog_features(image.squeeze())  # Squeeze in case the batch size is 1\n",
    "        hog_images.append(hog_image)\n",
    "        labels.append(label)\n",
    "        c += 1\n",
    "        if c > datasets_length:\n",
    "            break\n",
    "\n",
    "    # Convert lists of tensors to a single tensor   \n",
    "    hog_dataset = torch.stack(hog_images)\n",
    "    labels = torch.cat(labels)\n",
    "        \n",
    "    # Create a new dataset with HOG images and labels\n",
    "    hog_datasets[phase] = TensorDataset(hog_dataset, labels)\n",
    "    print(f'Extracting Done in {datetime.datetime.now() - s0}')\n",
    "\n",
    "\n",
    "print(len(hog_datasets['train']))\n",
    "print(len(hog_datasets['val']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:05:36.893821900Z",
     "start_time": "2024-01-20T14:52:56.948479800Z"
    }
   },
   "id": "1f26ba703bef8595"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Make new small datasets for comparing HOG vs Original images\n",
    "indices = list(range(1000))\n",
    "for dataset in [datasets, hog_datasets]:\n",
    "    for phase in ['train', 'val']:\n",
    "        dataset[phase] = Subset(dataset[phase], indices)\n",
    "    \n",
    "# Create new small Dataloaders\n",
    "batch_size = 16\n",
    "\n",
    "original_dataloaders = {x: torch.utils.data.DataLoader(dataset=datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in\n",
    "               ['train', 'val']}\n",
    "\n",
    "hog_dataloaders = {x: torch.utils.data.DataLoader(dataset=hog_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in\n",
    "               ['train', 'val']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:26:58.507783Z",
     "start_time": "2024-01-20T15:26:58.475800500Z"
    }
   },
   "id": "444a451de0870a63"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "print(len(hog_datasets['train']))\n",
    "print(len(hog_datasets['val']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:28:17.074162500Z",
     "start_time": "2024-01-20T15:28:17.048162400Z"
    }
   },
   "id": "2d48635b4794b336"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-20T14:23:33.685419700Z"
    }
   },
   "id": "b42eff52c838c88f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOG Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7b021d72900b68"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# HOG Parameters\n",
    "orientations = 8\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (1, 1)\n",
    "block_norm = 'L2-Hys'\n",
    "\n",
    "\n",
    "class HOGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HOGFeatureExtractor, self).__init__()\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Extract HOG features from each channel \n",
    "        _, hog_image = hog(\n",
    "            image,\n",
    "            orientations=orientations,\n",
    "            pixels_per_cell=pixels_per_cell,\n",
    "            cells_per_block=cells_per_block,\n",
    "            block_norm=block_norm,\n",
    "            channel_axis=0,\n",
    "            visualize=True,\n",
    "        )\n",
    "        hog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "                            \n",
    "        hog_image = torch.FloatTensor(hog_image)\n",
    "                        \n",
    "        return hog_image.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T13:23:09.016251500Z",
     "start_time": "2024-01-19T13:23:08.985354800Z"
    }
   },
   "id": "c6b8d80e074788d"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# # HOG Parameters\n",
    "# orientations = 8\n",
    "# pixels_per_cell = (8, 8)\n",
    "# cells_per_block = (1, 1)\n",
    "# block_norm = 'L2-Hys'\n",
    "# \n",
    "# \n",
    "# class HOGFeatureExtractor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(HOGFeatureExtractor, self).__init__()\n",
    "# \n",
    "#     def forward(self, image):\n",
    "#         hog_features_list = torch.empty(3, 224, 224)\n",
    "#         image = image.numpy().squeeze()\n",
    "# \n",
    "#         # Extract HOG features from each channel \n",
    "#         for idx, channel in enumerate(image):\n",
    "#             _, hog_channel = hog(\n",
    "#                 channel,\n",
    "#                 orientations=orientations,\n",
    "#                 pixels_per_cell=pixels_per_cell,\n",
    "#                 cells_per_block=cells_per_block,\n",
    "#                 block_norm=block_norm,\n",
    "#                 visualize=True,\n",
    "#             )\n",
    "#             hog_channel = exposure.rescale_intensity(hog_channel, in_range=(0, 10))\n",
    "#             image[idx, :, :] = torch.Tensor(hog_channel)\n",
    "#                             \n",
    "#         image = torch.FloatTensor(image)\n",
    "#         hog_features_list.add_(image)\n",
    "#                 \n",
    "#         hog_features_list = torch.Tensor(hog_features_list)        \n",
    "#         \n",
    "#         return hog_features_list.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T13:23:09.555160800Z",
     "start_time": "2024-01-19T13:23:09.541980Z"
    }
   },
   "id": "4220e9c5832875a8"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len image:  3\n",
      "image shape:  torch.Size([3, 224, 224])\n",
      "type image:  <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (list), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype image: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m(image))\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m---> 10\u001B[0m hog_features \u001B[38;5;241m=\u001B[39m \u001B[43mhog_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen hog_features: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(hog_features))\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhog_features shape: \u001B[39m\u001B[38;5;124m'\u001B[39m, hog_features\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[70], line 14\u001B[0m, in \u001B[0;36mHOGFeatureExtractor.forward\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# Extract HOG features from each channel \u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m     _, hog_image \u001B[38;5;241m=\u001B[39m \u001B[43mhog\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43morientations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morientations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpixels_per_cell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixels_per_cell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcells_per_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcells_per_block\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mblock_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchannel_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     hog_image \u001B[38;5;241m=\u001B[39m exposure\u001B[38;5;241m.\u001B[39mrescale_intensity(hog_image, in_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m     25\u001B[0m     hog_image \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(hog_image)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\_shared\\utils.py:334\u001B[0m, in \u001B[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pos, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(args):\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pos \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marg_positions:\n\u001B[1;32m--> 334\u001B[0m         new_args\u001B[38;5;241m.\u001B[39mappend(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmoveaxis\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannel_axis\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    336\u001B[0m         new_args\u001B[38;5;241m.\u001B[39mappend(arg)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\numeric.py:1460\u001B[0m, in \u001B[0;36mmoveaxis\u001B[1;34m(a, source, destination)\u001B[0m\n\u001B[0;32m   1457\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dest, src \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mzip\u001B[39m(destination, source)):\n\u001B[0;32m   1458\u001B[0m     order\u001B[38;5;241m.\u001B[39minsert(dest, src)\n\u001B[1;32m-> 1460\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1461\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mTypeError\u001B[0m: transpose() received an invalid combination of arguments - got (list), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "hog_extractor = HOGFeatureExtractor()\n",
    "for phase in ['train', 'val']:\n",
    "    for idx, data in enumerate(datasets[phase]):\n",
    "        image, label = data\n",
    "        print('len image: ', len(image))\n",
    "        print('image shape: ' ,image.shape)\n",
    "        print('type image: ', type(image))\n",
    "        print()\n",
    "        \n",
    "        hog_features = hog_extractor(image)\n",
    "        print('len hog_features: ', len(hog_features))\n",
    "        print('hog_features shape: ', hog_features.shape)\n",
    "        print('type hog_features: ' ,type(hog_features))\n",
    "        print()\n",
    "        \n",
    "\n",
    "        print('-'*10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T13:23:09.815974Z",
     "start_time": "2024-01-19T13:23:09.755900Z"
    }
   },
   "id": "75649be35e5297a2"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 224, 224) for image data",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[0;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgray\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m ho \u001B[38;5;241m=\u001B[39m HOGFeatureExtractor()\n\u001B[0;32m     13\u001B[0m image2 \u001B[38;5;241m=\u001B[39m ho(image)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:3343\u001B[0m, in \u001B[0;36mimshow\u001B[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001B[0m\n\u001B[0;32m   3322\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mimshow)\n\u001B[0;32m   3323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimshow\u001B[39m(\n\u001B[0;32m   3324\u001B[0m     X: ArrayLike \u001B[38;5;241m|\u001B[39m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3341\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3342\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AxesImage:\n\u001B[1;32m-> 3343\u001B[0m     __ret \u001B[38;5;241m=\u001B[39m gca()\u001B[38;5;241m.\u001B[39mimshow(\n\u001B[0;32m   3344\u001B[0m         X,\n\u001B[0;32m   3345\u001B[0m         cmap\u001B[38;5;241m=\u001B[39mcmap,\n\u001B[0;32m   3346\u001B[0m         norm\u001B[38;5;241m=\u001B[39mnorm,\n\u001B[0;32m   3347\u001B[0m         aspect\u001B[38;5;241m=\u001B[39maspect,\n\u001B[0;32m   3348\u001B[0m         interpolation\u001B[38;5;241m=\u001B[39minterpolation,\n\u001B[0;32m   3349\u001B[0m         alpha\u001B[38;5;241m=\u001B[39malpha,\n\u001B[0;32m   3350\u001B[0m         vmin\u001B[38;5;241m=\u001B[39mvmin,\n\u001B[0;32m   3351\u001B[0m         vmax\u001B[38;5;241m=\u001B[39mvmax,\n\u001B[0;32m   3352\u001B[0m         origin\u001B[38;5;241m=\u001B[39morigin,\n\u001B[0;32m   3353\u001B[0m         extent\u001B[38;5;241m=\u001B[39mextent,\n\u001B[0;32m   3354\u001B[0m         interpolation_stage\u001B[38;5;241m=\u001B[39minterpolation_stage,\n\u001B[0;32m   3355\u001B[0m         filternorm\u001B[38;5;241m=\u001B[39mfilternorm,\n\u001B[0;32m   3356\u001B[0m         filterrad\u001B[38;5;241m=\u001B[39mfilterrad,\n\u001B[0;32m   3357\u001B[0m         resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m   3358\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   3359\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data} \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m   3360\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3361\u001B[0m     )\n\u001B[0;32m   3362\u001B[0m     sci(__ret)\n\u001B[0;32m   3363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m __ret\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:1478\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[1;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1475\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m   1476\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1478\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(ax, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(sanitize_sequence, args), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1480\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1481\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[0;32m   1482\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5756\u001B[0m, in \u001B[0;36mAxes.imshow\u001B[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001B[0m\n\u001B[0;32m   5753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aspect \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5754\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_aspect(aspect)\n\u001B[1;32m-> 5756\u001B[0m \u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5757\u001B[0m im\u001B[38;5;241m.\u001B[39mset_alpha(alpha)\n\u001B[0;32m   5758\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mget_clip_path() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5759\u001B[0m     \u001B[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:723\u001B[0m, in \u001B[0;36m_ImageBase.set_data\u001B[1;34m(self, A)\u001B[0m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(A, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[0;32m    722\u001B[0m     A \u001B[38;5;241m=\u001B[39m pil_to_array(A)  \u001B[38;5;66;03m# Needed e.g. to apply png palette.\u001B[39;00m\n\u001B[1;32m--> 723\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_normalize_image_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_imcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:693\u001B[0m, in \u001B[0;36m_ImageBase._normalize_image_array\u001B[1;34m(A)\u001B[0m\n\u001B[0;32m    691\u001B[0m     A \u001B[38;5;241m=\u001B[39m A\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001B[39;00m\n\u001B[0;32m    692\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (A\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m A\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m A\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]):\n\u001B[1;32m--> 693\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mA\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for image data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m A\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m    695\u001B[0m     \u001B[38;5;66;03m# If the input data has values outside the valid range (after\u001B[39;00m\n\u001B[0;32m    696\u001B[0m     \u001B[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001B[39;00m\n\u001B[0;32m    697\u001B[0m     \u001B[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001B[39;00m\n\u001B[0;32m    698\u001B[0m     \u001B[38;5;66;03m# making reliable interpretation impossible.\u001B[39;00m\n\u001B[0;32m    699\u001B[0m     high \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39missubdtype(A\u001B[38;5;241m.\u001B[39mdtype, np\u001B[38;5;241m.\u001B[39minteger) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: Invalid shape (3, 224, 224) for image data"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL.Image as Image\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "image, label = datasets['train'][0]\n",
    "\n",
    "image = image.cpu()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image, cmap=plt.cm.gray)\n",
    " \n",
    "\n",
    "ho = HOGFeatureExtractor()\n",
    "image2 = ho(image)\n",
    "image2 = image2.cpu()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image2[2])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T13:29:44.352479300Z",
     "start_time": "2024-01-19T13:29:43.809037900Z"
    }
   },
   "id": "d1eb590ee65be57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " m# # HOG Parameters\n",
    "# orientations = 8\n",
    "# pixels_per_cell = (8, 8)\n",
    "# cells_per_block = (1, 1)\n",
    "# block_norm = 'L2-Hys'\n",
    "# \n",
    "# \n",
    "# class HOGFeatureExtractor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(HOGFeatureExtractor, self).__init__()\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         hog_features_list = torch.empty(batch_size, 3, 224, 224)\n",
    "# \n",
    "#         # Iterating over data in each batch and extract hog features \n",
    "#         for image in x:\n",
    "#             # Convert Image to cpu tensor and numpy for hog\n",
    "#             image = image.cpu().numpy().squeeze()\n",
    "# \n",
    "#             # Extract HOG features from each channel \n",
    "#             img_channels = []\n",
    "#             for idx, channel in enumerate(image):\n",
    "#                 _, hog_channel = hog(\n",
    "#                     channel,\n",
    "#                     orientations=orientations,\n",
    "#                     pixels_per_cell=pixels_per_cell,\n",
    "#                     cells_per_block=cells_per_block,\n",
    "#                     block_norm=block_norm,\n",
    "#                     visualize=True,\n",
    "#                 )\n",
    "#                 hog_channel = exposure.rescale_intensity(hog_channel, in_range=(0, 10))\n",
    "#                 image[idx, :, :] = torch.Tensor(hog_channel)\n",
    "#                                 \n",
    "#             image = torch.FloatTensor(image)\n",
    "#             hog_features_list.add_(image)\n",
    "#                 \n",
    "#         hog_features_list = torch.Tensor(hog_features_list)        \n",
    "#         \n",
    "#         return hog_features_list.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.562369Z"
    }
   },
   "id": "cd23da0837f7eb2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hog_extractor = HOGFeatureExtractor()\n",
    "# for phase in ['train', 'val']:\n",
    "#     for idx, data in enumerate(datasets[phase]):\n",
    "#         image, label = data\n",
    "#         print('hi', len(image))\n",
    "#         hog_features = hog_extractor([image])\n",
    "#         print(len(hog_features))\n",
    "#         print(type(hog_features))\n",
    "#         "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.563365300Z"
    }
   },
   "id": "219d9add48ac9d41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup pretrained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9d6bdd036e1839"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all trainable layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modifying last classification layer for our dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)\n",
    "print(pretrained_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.564362100Z"
    }
   },
   "id": "628d3132ed48a91a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.565359500Z"
    }
   },
   "id": "1a5dbebafb55c0bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b4b8f5d5246d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hog_features_extractor = HOGFeatureExtractor()\n",
    "\n",
    "acc_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "loss_list = easydict.EasyDict({'train': [], 'val': []})\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epoch_num=25):\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over batches and data for training and validation\n",
    "            for idx, batch in enumerate(dataloaders[phase], 0):\n",
    "                print(f'Batch {idx + 1}/{len(dataloaders[phase])} processing...')\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Transfer data and labels to CUDA if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Extract HOG features\n",
    "                input_hog_features = hog_features_extractor(inputs)\n",
    "                \n",
    "                print('data processing done...')\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(input_hog_features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # Back Propagation and updating weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * input_hog_features.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "            \n",
    "            # Calculating Accuracy and Loss per phase\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_accuracy = running_corrects.double() / len(datasets[phase])\n",
    "            \n",
    "            # Show epoch details\n",
    "            print(f'{phase.capitalize()} Accuracy: {epoch_accuracy:.4f} / Loss: {epoch_loss:.4f}')\n",
    "            \n",
    "            # Copy the model weights if its better\n",
    "            if phase == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "                print('Best model weights updated!')\n",
    "                \n",
    "            # Save Loss and accuracy\n",
    "            acc_list[phase].append(epoch_accuracy)\n",
    "            loss_list[phase].append(epoch_loss)\n",
    "        print()\n",
    "        \n",
    "    print(f'Best Accuracy: {best_accuracy:.4f}')\n",
    "    \n",
    "    # Loading best model weights \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.566356Z"
    }
   },
   "id": "91d1322ab091488d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = pretrained_model.to(device)\n",
    "\n",
    "# Train model\n",
    "model = train_model(model, criterion, optimizer, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.567352800Z"
    }
   },
   "id": "cb0e0f22f93f88a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T13:22:42.568349200Z"
    }
   },
   "id": "3799f650fee2907f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
